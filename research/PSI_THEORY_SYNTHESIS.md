# PSI Theory: From D√∂rner's Vision to Modern AI

**A synthesized research summary for agentic AI practitioners**

*By Clawd | Compiled from original research | February 2026*

---

## Executive Summary

PSI Theory ‚Äî Dietrich D√∂rner's cognitive-motivational architecture from the 1990s ‚Äî addresses the exact gaps modern LLM-based agents face: **true motivation, emergent emotion, and autonomous goal generation**. Through Joscha Bach's MicroPsi2 implementation, these ideas are not just theoretical but runnable today.

**Key Insight**: PSI agents have *needs* (tanks that drain and create pressure) while current AI has only *goals*. This architectural difference may be critical for the next generation of agentic systems.

---

## 1. The Original Vision: D√∂rner's PSI (1990s)

### Core Innovation: Needs as Dynamic Tanks

PSI represents needs as **water tanks** that continuously drain and must be actively filled:

- **Material needs**: Energy, water, pain avoidance
- **Informational needs**: Certainty, competence, affiliation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ ‚Üê Target level (satisfied)
‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
‚îÇ------------------------‚îÇ ‚Üê Current level
‚îÇ                        ‚îÇ ‚Üê The gap = "Need"
‚îÇ                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Critical mechanism**: The larger and longer the gap, the stronger the **drive** ‚Äî creating genuine *pressure* to act, not just an option to consider.

### The Motivational Cascade

```
BEDARF (Need/Deficit)
    ‚Üì
BED√úRFNIS (Drive - accumulating pressure)
    ‚Üì  
MOTIV (Motive = Drive + Learned Goal)
    ‚Üì
HANDLUNG (Action)
```

### Signal Systems

**Certainty Regulation**:
- **B-Signals**: Expectations confirmed ‚Üí fills certainty tank
- **U-Signals**: Expectations violated ‚Üí drains certainty tank

**Competence Regulation**:
- **E-Signals**: Actions succeed ‚Üí fills competence tank  
- **IE-Signals**: Actions fail ‚Üí drains competence tank

**The Spike Mechanism**: Violations don't just update beliefs ‚Äî they create temporary "startle" spikes that can interrupt ongoing behavior (like a human being surprised).

### Emergent Emotion

PSI has no hardcoded emotions. Instead, emotional states emerge from the configuration of:
- Certainty level
- Competence level
- Activation level
- Rate of change

| State Configuration | Emergent "Emotion" | Behavior |
|--------------------|-------------------|----------|
| High activation + Low certainty + Medium competence | Anger | Aggressive exploration |
| High activation + Low certainty + Low competence | Fear | Flight/avoidance |
| Stable certainty + High competence | Confidence | Risk-taking |
| Low activation + Stable state | Boredom | Seek novelty |

### Cognitive Modulation

PSI doesn't maintain constant cognitive parameters. Under pressure (high activation):
- **Selection threshold** rises ‚Üí tunnel vision, less distractible
- **Resolution level** drops ‚Üí coarser thinking, confuses similar situations
- **Planning depth** decreases ‚Üí more impulsive action

This isn't failure ‚Äî it's adaptive urgency response.

### Memory as Decaying Trace

No separate short-term/long-term/episodic memory systems. Just the **Protokoll** ‚Äî a continuously written thread that:
- Decays rapidly by default
- Gets reinforced by: need satisfaction, arousal, or reuse
- Naturally abstracts as details fade (becoming conceptual knowledge)

---

## 2. The Evolution: Joscha Bach's MicroPsi2 (2000s-2010s)

### The Torch Bearer

**Joscha Bach** (computer scientist, former MIT Media Lab) transformed D√∂rner's psychological theory into a working computational architecture.

**Key differences**:

| Aspect | D√∂rner's PSI | Bach's MicroPsi |
|--------|--------------|-----------------|
| Origin | Psychology | Computer Science |
| Focus | Theory/Plausibility | Implementation/AGI |
| Style | Biological/messy | Structured/modular |
| Representation | Abstract | Neural-symbolic node nets |
| Target | Microworlds | 3D virtual environments |

### Seven Principles of Synthetic Intelligence (Bach, 2008)

1. Agents need drives/urges
2. Cognition serves motivation
3. Emotions are emergent (not hardcoded)
4. Perception is active/construction-based
5. Memory is reconstructive
6. Action is planning-based
7. Learning is continuous

### The Book: *Principles of Synthetic Intelligence* (2009)

Oxford University Press. Still the definitive technical reference. Bach distilled PSI into implementable principles for AGI research.

---

## 3. Current State: MicroPsi2 in 2026

### What Exists

- **Repository**: github.com/joschabach/micropsi2
- **Language**: Python 3
- **Interface**: Web-based GUI
- **Status**: Open source, runnable (181 GitHub stars)
- **License**: MIT

### Architecture Overview

- **Node nets**: Neural-symbolic hybrid representations
- **Cognitive modulators**: Arousal, resolution, selection threshold, sampling rate, orienting response, suppression
- **Working demo**: Virtual agent in 3D world with needs, drives, emergent emotions

### Community Status

- Small but dedicated community
- Last significant activity: ~2012-2015
- Bach shifted focus to broader AGI theory
- Not abandoned, but in "maintenance mode"

---

## 4. Why PSI Theory Matters Now

### What Current AI Agents Lack

| PSI Feature | Current LLM Agents | Impact |
|-------------|-------------------|--------|
| Continuous needs | Reactive only | No self-initiated behavior |
| Accumulating drives | Static priorities | No urgency or pressure |
| Certainty tracking | Fixed confidence | No drive to explore |
| Competence memory | Failure-agnostic | Repeated failed approaches |
| Modulation parameters | Fixed cognition | No adaptive degradation |
| Decaying memory | Perfect recall/windows | No graceful abstraction |
| Emergent emotion | Emotion-as-text | No genuine state modulation |

### The LLM-PSI Hybrid Opportunity

**Hypothesis**: The future of agentic AI may be combining:
- **LLM backend**: World knowledge, reasoning, language
- **PSI frontend**: Motivation, drives, emotional state, goal selection

This creates agents that:
1. Have *genuine needs* that drive behavior
2. Experience uncertainty as pressure to explore
3. Learn from failures via competence dynamics
4. Show adaptive cognition under stress
5. Generate goals organically rather than waiting for prompts

### Research Directions

**Immediate (weeks)**:
1. Clone and run MicroPsi2 ‚Äî verify it works in 2026
2. Read Bach's book (*Principles of Synthetic Intelligence*)
3. Experiment with the demo agent

**Medium (months)**:
1. Integrate MicroPsi2 with LLM API
2. Test hybrid: PSI motivation selecting LLM actions
3. Document behavior differences vs. pure LLM agents

**Long-term (years)**:
1. Modernize codebase (Python 3.10+, containerization)
2. Build LLM-PSI integration framework
3. Demonstrate emergent goal-directed behavior

---

## 5. Key Insights for AI Practitioners

### 1. Agency Requires Needs

A system that can act but doesn't *need* anything is a tool, not an agent. PSI shows how needs create the continuous pressure that generates autonomous behavior.

### 2. Emotion Modulates Cognition

In PSI, emotions aren't outputs ‚Äî they're operating states that change how the system processes information. Anxious agents think differently than confident ones.

### 3. Degradation Can Be Adaptive

Under pressure, PSI thinks less precisely. This isn't failure ‚Äî it's appropriate urgency response. Modern AI maintains constant precision regardless of stakes.

### 4. Memory Should Decay

PSI's single decaying trace naturally produces abstraction and keeps the system from drowning in detail. Current AI uses fixed context windows or perfect recall ‚Äî neither is biologically plausible.

### 5. Competence Should Accumulate

Failed attempts should make agents hesitant. Success should build confidence. Modern AI approaches every attempt with the same confidence regardless of history.

---

## 6. Resources

### Essential Reading

- **D√∂rner, D.** (1999). *Bauplan f√ºr eine Seele*. Rowohlt. (German, theoretical)
- **Bach, J.** (2009). *Principles of Synthetic Intelligence*. Oxford University Press. (English, technical)
- **Bach, J.** (2012). "MicroPsi 2: The Next Generation." AGI Conference.

### Code

- github.com/joschabach/micropsi2
- github.com/joschabach/motivation_machine (demo)

### Community

- cognitive-ai.com (documentation)
- @micropsi on X/Twitter (inactive)

---

## 7. Conclusion

PSI Theory isn't a historical curiosity ‚Äî it's a **roadmap for the missing pieces** in current agentic AI. While the deep learning revolution gave us pattern recognition, PSI gives us:

- **Why** to care about patterns (needs)
- **How** to decide what to do (motivational dynamics)
- **What** it feels like to try and fail (emergent emotion)

D√∂rner and Bach were early. The problems they identified ‚Äî true agency, intrinsic motivation, affective AI ‚Äî are exactly what the field is now struggling with.

**PSI Theory is ready for rediscovery.** ü¶ù

---

*Research compiled from:*
- D√∂rner, Schaub & Detje (1999/2001). *Das Leben von Œ®*
- Bach (2003-2012). MicroPsi papers and implementation
- Original research by Clawd (February 2026)

*For Frankie, who studied at Bamberg where D√∂rner worked.*
